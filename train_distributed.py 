# train_distributed.py - SageMaker Distributed Data Parallel Training

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR, CosineAnnealingWarmRestarts
from torch_geometric.loader import DataLoader
from torch.utils.data.distributed import DistributedSampler
import numpy as np
from tqdm.auto import tqdm
import os
import sys
import json
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
import pandas as pd
import argparse

# SageMaker Distributed Data Parallel imports
try:
    import smdistributed.dataparallel.torch.distributed as dist
    from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP
    SMDP_AVAILABLE = True
except ImportError:
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    SMDP_AVAILABLE = False
    print("SMDP not available, falling back to PyTorch DDP")

from config import Config
from data.neptune_extractor import NeptuneDataExtractor
from data.data_preprocessor import PackageLifecyclePreprocessor
from data.dataset import PackageLifecycleDataset
from config import ModelConfig
from models.event_predictor import EventTimePredictor
from utils.metrics import compute_metrics, EarlyStopping
from utils.package_filter import PackageEventValidator


def setup_distributed():
    """Setup distributed training environment"""
    if SMDP_AVAILABLE:
        # SageMaker Distributed Data Parallel
        dist.init_process_group()
        rank = dist.get_rank()
        local_rank = dist.get_local_rank()
        world_size = dist.get_world_size()
    else:
        # Fallback to PyTorch DDP
        dist.init_process_group(backend='nccl')
        rank = dist.get_rank()
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        world_size = dist.get_world_size()
    
    torch.cuda.set_device(local_rank)
    device = torch.device('cuda', local_rank)
    
    return rank, local_rank, world_size, device


def cleanup_distributed():
    """Cleanup distributed training"""
    dist.destroy_process_group()


def is_main_process(rank):
    """Check if this is the main process"""
    return rank == 0


def print_rank0(msg, rank):
    """Print only from rank 0"""
    if is_main_process(rank):
        print(msg)


def set_seed(seed: int, rank: int = 0):
    """Set random seeds for reproducibility"""
    seed = seed + rank  # Different seed per rank for data augmentation diversity
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def load_data(config, rank) -> pd.DataFrame:
    """Load data from source"""
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 1: Load data", rank)
    print_rank0("="*80, rank)
    
    # In SageMaker, data is typically in /opt/ml/input/data/
    data_path = getattr(config.data, 'source_file', 'data/graph-data/package_lifecycles_batch_2.json')
    
    # Check for SageMaker data path
    sagemaker_data_path = os.environ.get('SM_CHANNEL_TRAINING', None)
    if sagemaker_data_path:
        data_path = os.path.join(sagemaker_data_path, os.path.basename(data_path))
    
    df = pd.read_json(data_path)
    
    print_rank0(f"Loaded {len(df)} package lifecycles from {data_path}", rank)
    
    if 'events' in df.columns:
        event_counts = df['events'].apply(len)
        print_rank0(f"Events per package: min={event_counts.min()}, max={event_counts.max()}, mean={event_counts.mean():.1f}", rank)
    
    return df


def split_data(df: pd.DataFrame, config, rank) -> tuple:
    """Split data into train/val/test sets"""
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 2: Split data", rank)
    print_rank0("="*80, rank)
    
    # Shuffle data with fixed seed for consistency across ranks
    df = df.sample(frac=1, random_state=config.seed).reset_index(drop=True)
    
    train_ratio = getattr(config.data, 'train_ratio', 0.8)
    val_ratio = getattr(config.data, 'val_ratio', 0.1)
    
    train_size = int(train_ratio * len(df))
    val_size = int(val_ratio * len(df))
    
    train_df = df.iloc[:train_size].reset_index(drop=True)
    val_df = df.iloc[train_size:train_size+val_size].reset_index(drop=True)
    test_df = df.iloc[train_size+val_size:].reset_index(drop=True)
    
    print_rank0(f"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)", rank)
    print_rank0(f"Val: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)", rank)
    print_rank0(f"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)", rank)
    
    return train_df, val_df, test_df


def create_preprocessor(config, train_df: pd.DataFrame, rank) -> PackageLifecyclePreprocessor:
    """Create and fit preprocessor on training data"""
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 3: Fit preprocessor", rank)
    print_rank0("="*80, rank)
    
    distance_file = getattr(config.data, 'distance_file', 'data/location_distances_complete.csv')
    
    # Check for SageMaker data path
    sagemaker_data_path = os.environ.get('SM_CHANNEL_TRAINING', None)
    if sagemaker_data_path and os.path.exists(os.path.join(sagemaker_data_path, os.path.basename(distance_file))):
        distance_file = os.path.join(sagemaker_data_path, os.path.basename(distance_file))
    
    preprocessor = PackageLifecyclePreprocessor(
        config=config,
        distance_file_path=distance_file
    )
    
    preprocessor.fit(train_df)
    
    if is_main_process(rank):
        feature_dims = preprocessor.get_feature_dimensions()
        print(f"\nFeature dimensions:")
        for key, value in feature_dims.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")
        
        dist_coverage = preprocessor.get_distance_coverage()
        print(f"\nDistance coverage:")
        for key, value in dist_coverage.items():
            print(f"  {key}: {value}")
    
    return preprocessor


def create_distributed_dataloaders(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame,
                                    preprocessor: PackageLifecyclePreprocessor, config, 
                                    rank, world_size) -> tuple:
    """Create distributed PyG DataLoaders"""
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 4: Create distributed datasets and dataloaders", rank)
    print_rank0("="*80, rank)
    
    # Create datasets
    print_rank0("\nCreating training dataset...", rank)
    train_dataset = PackageLifecycleDataset(
        train_df, preprocessor, 
        return_labels=True,
    )
    
    print_rank0("\nCreating validation dataset...", rank)
    val_dataset = PackageLifecycleDataset(
        val_df, preprocessor, 
        return_labels=True,
    )
    
    print_rank0("\nCreating test dataset...", rank)
    test_dataset = PackageLifecycleDataset(
        test_df, preprocessor, 
        return_labels=True,
    )
    
    print_rank0(f"\nTrain dataset: {len(train_dataset)} samples", rank)
    print_rank0(f"Val dataset: {len(val_dataset)} samples", rank)
    print_rank0(f"Test dataset: {len(test_dataset)} samples", rank)
    
    # Get batch size - scale per GPU
    batch_size = getattr(config.training, 'batch_size', 64)
    
    # Number of workers per process
    num_workers = max(1, os.cpu_count() // world_size - 1)
    
    # Create distributed samplers
    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True,
        seed=config.seed
    )
    
    val_sampler = DistributedSampler(
        val_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False
    )
    
    test_sampler = DistributedSampler(
        test_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False
    )
    
    # Create dataloaders with distributed samplers
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        sampler=train_sampler,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,  # Important for distributed training
        prefetch_factor=4 if num_workers > 0 else None,
        persistent_workers=True if num_workers > 0 else False,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size * 2,
        sampler=val_sampler,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=4 if num_workers > 0 else None,
        persistent_workers=True if num_workers > 0 else False,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size * 2,
        sampler=test_sampler,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=4 if num_workers > 0 else None,
        persistent_workers=True if num_workers > 0 else False,
    )
    
    print_rank0(f"\nDistributed DataLoader settings:", rank)
    print_rank0(f"  World size: {world_size}", rank)
    print_rank0(f"  Batch size per GPU: {batch_size}", rank)
    print_rank0(f"  Effective batch size: {batch_size * world_size}", rank)
    print_rank0(f"  Num workers per GPU: {num_workers}", rank)
    print_rank0(f"  Train batches per GPU: {len(train_loader)}", rank)
    print_rank0(f"  Val batches per GPU: {len(val_loader)}", rank)
    print_rank0(f"  Test batches per GPU: {len(test_loader)}", rank)
    
    return train_loader, val_loader, test_loader, train_sampler


def create_model(preprocessor: PackageLifecyclePreprocessor, config, device: torch.device, 
                 rank, world_size) -> EventTimePredictor:
    """Create model and wrap with DDP"""
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 5: Initialize distributed model", rank)
    print_rank0("="*80, rank)
    
    vocab_sizes = preprocessor.get_vocab_sizes()
    
    model_config = ModelConfig.from_preprocessor(
        preprocessor,
        hidden_dim=getattr(config.model, 'hidden_dim', 256),
        num_layers=getattr(config.model, 'num_layers', 20),
        num_heads=getattr(config.model, 'num_heads', 8),
        dropout=getattr(config.model, 'dropout', 0.1),
        embed_dim=getattr(config.model, 'embed_dim', 32),
        output_dim=getattr(config.model, 'output_dim', 1),
        use_edge_features=getattr(config.model, 'use_edge_features', True),
        use_global_attention=getattr(config.model, 'use_global_attention', True),
        use_positional_encoding=getattr(config.model, 'use_positional_encoding', True)
    )
    
    # Create model
    model = EventTimePredictor(model_config, vocab_sizes)
    model = model.to(device)
    
    # Wrap with DDP
    if SMDP_AVAILABLE:
        model = DDP(model)
    else:
        model = DDP(model, device_ids=[device.index], output_device=device.index)
    
    if is_main_process(rank):
        # Get parameters from the underlying model
        base_model = model.module if hasattr(model, 'module') else model
        params = base_model.get_num_parameters()
        print(f"\nModel parameters:")
        print(f"  Total: {params['total']:,}")
        print(f"  Trainable: {params['trainable']:,}")
        print(f"\nDistributed Training:")
        print(f"  World size: {world_size}")
        print(f"  Using SMDP: {SMDP_AVAILABLE}")
    
    return model, model_config, vocab_sizes


def create_optimizer_and_scheduler(model: nn.Module, config, num_training_steps: int, 
                                    world_size: int, rank) -> tuple:
    """Create optimizer and learning rate scheduler with scaled learning rate"""
    # Scale learning rate by world size (linear scaling rule)
    base_lr = getattr(config.training, 'learning_rate', 1e-4)
    learning_rate = base_lr * world_size  # Linear scaling
    weight_decay = getattr(config.training, 'weight_decay', 0.01)
    
    optimizer = AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    scheduler_type = getattr(config.training, 'scheduler_type', 'cosine')
    num_epochs = getattr(config.training, 'num_epochs', 100)
    
    scheduler = None
    
    if scheduler_type == 'cosine':
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=num_epochs,
            eta_min=learning_rate * 0.01
        )
    elif scheduler_type == 'cosine_warm_restarts':
        scheduler = CosineAnnealingWarmRestarts(
            optimizer,
            T_0=10,
            T_mult=2,
            eta_min=learning_rate * 0.01
        )
    elif scheduler_type == 'onecycle':
        scheduler = OneCycleLR(
            optimizer,
            max_lr=learning_rate,
            epochs=num_epochs,
            steps_per_epoch=num_training_steps,
            pct_start=0.1,
            anneal_strategy='cos',
            div_factor=25.0,
            final_div_factor=1000.0
        )
    
    print_rank0(f"\nOptimizer: AdamW (base_lr={base_lr}, scaled_lr={learning_rate}, weight_decay={weight_decay})", rank)
    print_rank0(f"Learning rate scaling: {world_size}x (linear scaling rule)", rank)
    print_rank0(f"Scheduler: {scheduler_type}", rank)
    
    return optimizer, scheduler, scheduler_type


def reduce_tensor(tensor, world_size):
    """Reduce tensor across all processes"""
    rt = tensor.clone()
    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
    rt /= world_size
    return rt


def gather_predictions(preds, targets, world_size):
    """Gather predictions from all processes"""
    # Get local sizes
    local_size = torch.tensor([preds.shape[0]], device=preds.device)
    sizes = [torch.zeros(1, device=preds.device, dtype=torch.long) for _ in range(world_size)]
    dist.all_gather(sizes, local_size)
    sizes = [int(s.item()) for s in sizes]
    max_size = max(sizes)
    
    # Pad tensors to same size
    if preds.shape[0] < max_size:
        padding = torch.zeros(max_size - preds.shape[0], device=preds.device)
        preds = torch.cat([preds, padding])
        targets = torch.cat([targets, padding])
    
    # Gather
    gathered_preds = [torch.zeros(max_size, device=preds.device) for _ in range(world_size)]
    gathered_targets = [torch.zeros(max_size, device=targets.device) for _ in range(world_size)]
    
    dist.all_gather(gathered_preds, preds)
    dist.all_gather(gathered_targets, targets)
    
    # Unpad and concatenate
    all_preds = []
    all_targets = []
    for i, size in enumerate(sizes):
        all_preds.append(gathered_preds[i][:size])
        all_targets.append(gathered_targets[i][:size])
    
    return torch.cat(all_preds), torch.cat(all_targets)


def train_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer,
                criterion: nn.Module, device: torch.device, preprocessor: PackageLifecyclePreprocessor,
                rank, world_size, scaler=None, scheduler=None, scheduler_type: str = None) -> dict:
    """Train for one epoch with distributed training"""
    model.train()
    
    total_loss = 0
    total_samples = 0
    all_preds_scaled = []
    all_targets_scaled = []
    
    # Progress bar only on rank 0
    if is_main_process(rank):
        pbar = tqdm(
            loader, 
            desc='Training', 
            leave=False, 
            position=0,
            file=sys.stdout,
            dynamic_ncols=True,
            mininterval=0.5
        )
    else:
        pbar = loader
    
    for batch in pbar:
        batch = batch.to(device)
        
        optimizer.zero_grad()
        
        # Forward pass with optional mixed precision
        if scaler is not None:
            with torch.amp.autocast('cuda'):
                predictions = model(batch)
                mask = batch.label_mask
                masked_preds = predictions[mask]
                masked_targets = batch.labels
                loss = criterion(masked_preds, masked_targets)
        else:
            predictions = model(batch)
            mask = batch.label_mask
            masked_preds = predictions[mask]
            masked_targets = batch.labels
            loss = criterion(masked_preds, masked_targets)
        
        # Backward pass
        if scaler is not None:
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        # Step scheduler if OneCycleLR
        if scheduler is not None and scheduler_type == 'onecycle':
            scheduler.step()
        
        # Track metrics (local)
        batch_size = masked_preds.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size
        
        all_preds_scaled.append(masked_preds.detach())
        all_targets_scaled.append(masked_targets.detach())
        
        if is_main_process(rank):
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{total_loss/total_samples:.4f}'
            })
    
    if is_main_process(rank):
        pbar.close()
    
    # Gather metrics across all processes
    all_preds_scaled = torch.cat(all_preds_scaled).squeeze()
    all_targets_scaled = torch.cat(all_targets_scaled).squeeze()
    
    # Reduce loss
    total_loss_tensor = torch.tensor([total_loss], device=device)
    total_samples_tensor = torch.tensor([total_samples], device=device)
    
    dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
    dist.all_reduce(total_samples_tensor, op=dist.ReduceOp.SUM)
    
    avg_loss = total_loss_tensor.item() / total_samples_tensor.item()
    
    # Gather all predictions for metrics (only on rank 0 to save memory)
    if is_main_process(rank):
        all_preds_gathered, all_targets_gathered = gather_predictions(
            all_preds_scaled, all_targets_scaled, world_size
        )
        
        preds_np = all_preds_gathered.cpu().numpy()
        targets_np = all_targets_gathered.cpu().numpy()
        
        metrics_scaled = compute_metrics(preds_np, targets_np)
        
        preds_hours = preprocessor.inverse_transform_time(preds_np)
        targets_hours = preprocessor.inverse_transform_time(targets_np)
        metrics_hours = compute_metrics(preds_hours, targets_hours)
        
        metrics = {
            'loss': avg_loss,
            'mae_scaled': metrics_scaled.get('mae', 0),
            'rmse_scaled': metrics_scaled.get('rmse', 0),
            'mae_hours': metrics_hours.get('mae', 0),
            'rmse_hours': metrics_hours.get('rmse', 0),
            'mape': metrics_hours.get('mape', 0),
            'r2': metrics_hours.get('r2', 0),
            'num_samples': int(total_samples_tensor.item())
        }
    else:
        metrics = {'loss': avg_loss, 'num_samples': int(total_samples_tensor.item())}
    
    return metrics


@torch.no_grad()
def validate(model: nn.Module, loader: DataLoader, criterion: nn.Module,
             device: torch.device, preprocessor: PackageLifecyclePreprocessor,
             rank, world_size) -> dict:
    """Validate model with distributed evaluation"""
    model.eval()
    
    total_loss = 0
    total_samples = 0
    all_preds_scaled = []
    all_targets_scaled = []
    
    if is_main_process(rank):
        pbar = tqdm(
            loader, 
            desc='Validating', 
            leave=False, 
            position=0,
            file=sys.stdout,
            dynamic_ncols=True,
            mininterval=0.5
        )
    else:
        pbar = loader
    
    for batch in pbar:
        batch = batch.to(device)
        
        predictions = model(batch)
        mask = batch.label_mask
        masked_preds = predictions[mask]
        masked_targets = batch.labels
        
        loss = criterion(masked_preds, masked_targets)
        
        batch_size = masked_preds.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size
        
        all_preds_scaled.append(masked_preds)
        all_targets_scaled.append(masked_targets)
        
        if is_main_process(rank):
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    if is_main_process(rank):
        pbar.close()
    
    # Gather metrics across all processes
    all_preds_scaled = torch.cat(all_preds_scaled).squeeze()
    all_targets_scaled = torch.cat(all_targets_scaled).squeeze()
    
    total_loss_tensor = torch.tensor([total_loss], device=device)
    total_samples_tensor = torch.tensor([total_samples], device=device)
    
    dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
    dist.all_reduce(total_samples_tensor, op=dist.ReduceOp.SUM)
    
    avg_loss = total_loss_tensor.item() / total_samples_tensor.item()
    
    if is_main_process(rank):
        all_preds_gathered, all_targets_gathered = gather_predictions(
            all_preds_scaled, all_targets_scaled, world_size
        )
        
        preds_np = all_preds_gathered.cpu().numpy()
        targets_np = all_targets_gathered.cpu().numpy()
        
        metrics_scaled = compute_metrics(preds_np, targets_np)
        
        preds_hours = preprocessor.inverse_transform_time(preds_np)
        targets_hours = preprocessor.inverse_transform_time(targets_np)
        metrics_hours = compute_metrics(preds_hours, targets_hours)
        
        metrics = {
            'loss': avg_loss,
            'mae_scaled': metrics_scaled.get('mae', 0),
            'rmse_scaled': metrics_scaled.get('rmse', 0),
            'mae_hours': metrics_hours.get('mae', 0),
            'rmse_hours': metrics_hours.get('rmse', 0),
            'mape': metrics_hours.get('mape', 0),
            'r2': metrics_hours.get('r2', 0),
            'num_samples': int(total_samples_tensor.item())
        }
    else:
        metrics = {'loss': avg_loss, 'num_samples': int(total_samples_tensor.item())}
    
    # Broadcast metrics to all processes for early stopping
    metrics_tensor = torch.tensor([avg_loss], device=device)
    dist.broadcast(metrics_tensor, src=0)
    metrics['loss'] = metrics_tensor.item()
    
    return metrics


def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,
                    scheduler, epoch: int, metrics: dict,
                    model_config: ModelConfig, vocab_sizes: dict,
                    save_path: str, rank: int):
    """Save model checkpoint (only from rank 0)"""
    if not is_main_process(rank):
        return
    
    # Get the underlying model from DDP wrapper
    model_to_save = model.module if hasattr(model, 'module') else model
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model_to_save.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'model_config': model_config.to_dict(),
        'vocab_sizes': vocab_sizes,
        'metrics': metrics
    }
    torch.save(checkpoint, save_path)


def load_checkpoint(checkpoint_path: str, model: nn.Module, optimizer: torch.optim.Optimizer = None,
                    scheduler = None, device: torch.device = None) -> dict:
    """Load model checkpoint"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # Handle DDP wrapper
    model_to_load = model.module if hasattr(model, 'module') else model
    model_to_load.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler is not None and checkpoint.get('scheduler_state_dict'):
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    return checkpoint


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Distributed Training for Package Event Prediction')
    
    # SageMaker specific arguments
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR', './model'))
    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR', './output'))
    parser.add_argument('--data-dir', type=str, default=os.environ.get('SM_CHANNEL_TRAINING', './data'))
    
    # Training arguments
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--learning-rate', type=float, default=1e-4)
    parser.add_argument('--seed', type=int, default=42)
    
    return parser.parse_args()


def main():
    """Main distributed training function"""
    
    # Parse arguments
    args = parse_args()
    
    # ========================================
    # Setup Distributed Training
    # ========================================
    
    rank, local_rank, world_size, device = setup_distributed()
    
    # Load configuration
    config = Config()
    
    # Override config with args
    config.training.num_epochs = args.epochs
    config.training.batch_size = args.batch_size
    config.training.learning_rate = args.learning_rate
    config.seed = args.seed
    
    set_seed(config.seed, rank)
    
    # Create save directories (only on rank 0)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Use SageMaker model directory
    save_dir = args.model_dir
    log_dir = os.path.join(args.output_data_dir, 'logs', timestamp)
    
    if is_main_process(rank):
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
    
    # Synchronize before continuing
    dist.barrier()
    
    print_rank0("="*80, rank)
    print_rank0("Package Event Time Prediction - Distributed Training", rank)
    print_rank0("="*80, rank)
    print_rank0(f"Timestamp: {timestamp}", rank)
    print_rank0(f"Save directory: {save_dir}", rank)
    print_rank0(f"Log directory: {log_dir}", rank)
    print_rank0(f"Seed: {config.seed}", rank)
    print_rank0(f"World size: {world_size}", rank)
    print_rank0(f"Rank: {rank}, Local rank: {local_rank}", rank)
    print_rank0(f"Device: {device}", rank)
    print_rank0(f"Using SMDP: {SMDP_AVAILABLE}", rank)
    
    if device.type == 'cuda':
        print_rank0(f"GPU: {torch.cuda.get_device_name(local_rank)}", rank)
        print_rank0(f"GPU Memory: {torch.cuda.get_device_properties(local_rank).total_memory / 1e9:.1f} GB", rank)
    
    # ========================================
    # Data Loading and Preprocessing
    # ========================================
    
    df = load_data(config, rank)
    train_df, val_df, test_df = split_data(df, config, rank)
    preprocessor = create_preprocessor(config, train_df, rank)
    
    train_loader, val_loader, test_loader, train_sampler = create_distributed_dataloaders(
        train_df, val_df, test_df, preprocessor, config, rank, world_size
    )
    
    # ========================================
    # Model Setup
    # ========================================
    
    model, model_config, vocab_sizes = create_model(preprocessor, config, device, rank, world_size)
    
    num_training_steps = len(train_loader)
    optimizer, scheduler, scheduler_type = create_optimizer_and_scheduler(
        model, config, num_training_steps, world_size, rank
    )
    
    criterion = nn.MSELoss()
    
    patience = getattr(config.training, 'patience', 15)
    min_delta = getattr(config.training, 'min_delta', 1e-4)
    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)
    
    # Tensorboard writer (only on rank 0)
    writer = None
    if is_main_process(rank):
        writer = SummaryWriter(log_dir)
    
    use_amp = getattr(config.training, 'use_amp', True) and device.type == 'cuda'
    scaler = torch.amp.GradScaler('cuda') if use_amp else None
    
    print_rank0(f"Using mixed precision training (AMP): {use_amp}", rank)
    
    # ========================================
    # Save Configuration (rank 0 only)
    # ========================================
    
    if is_main_process(rank):
        config_dict = {
            'timestamp': timestamp,
            'seed': config.seed,
            'world_size': world_size,
            'model_config': model_config.to_dict(),
            'vocab_sizes': vocab_sizes,
            'feature_dims': preprocessor.get_feature_dimensions(),
            'training': {
                'batch_size_per_gpu': getattr(config.training, 'batch_size', 32),
                'effective_batch_size': getattr(config.training, 'batch_size', 32) * world_size,
                'base_learning_rate': getattr(config.training, 'learning_rate', 1e-4),
                'scaled_learning_rate': getattr(config.training, 'learning_rate', 1e-4) * world_size,
                'weight_decay': getattr(config.training, 'weight_decay', 0.01),
                'num_epochs': getattr(config.training, 'num_epochs', 100),
                'scheduler_type': scheduler_type,
                'patience': patience,
                'min_delta': min_delta,
                'use_amp': use_amp,
            },
            'data': {
                'train_size': len(train_df),
                'val_size': len(val_df),
                'test_size': len(test_df),
            },
            'distributed': {
                'world_size': world_size,
                'using_smdp': SMDP_AVAILABLE,
            }
        }
        
        with open(os.path.join(save_dir, 'config.json'), 'w') as f:
            json.dump(config_dict, f, indent=2, default=str)
        
        preprocessor.save(os.path.join(save_dir, 'preprocessor.pkl'))
    
    # Synchronize
    dist.barrier()
    
    # ========================================
    # Training Loop
    # ========================================
    
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 6: Distributed Training", rank)
    print_rank0("="*80, rank)
    
    num_epochs = getattr(config.training, 'num_epochs', 100)
    save_every = getattr(config.training, 'save_every', 10)
    
    best_val_loss = float('inf')
    best_val_mae_hours = float('inf')
    best_epoch = 0
    
    training_history = {
        'train_loss': [],
        'val_loss': [],
        'train_mae_hours': [],
        'val_mae_hours': [],
        'train_rmse_hours': [],
        'val_rmse_hours': [],
        'val_r2': [],
        'learning_rate': []
    }
    
    for epoch in range(num_epochs):
        epoch_start_time = datetime.now()
        
        # Set epoch for distributed sampler (important for shuffling)
        train_sampler.set_epoch(epoch)
        
        print_rank0(f"\nEpoch {epoch+1}/{num_epochs}", rank)
        print_rank0("-" * 50, rank)
        
        current_lr = optimizer.param_groups[0]['lr']
        print_rank0(f"Learning rate: {current_lr:.2e}", rank)
        
        # Train
        train_metrics = train_epoch(
            model=model,
            loader=train_loader,
            optimizer=optimizer,
            criterion=criterion,
            device=device,
            preprocessor=preprocessor,
            rank=rank,
            world_size=world_size,
            scaler=scaler,
            scheduler=scheduler if scheduler_type == 'onecycle' else None,
            scheduler_type=scheduler_type
        )
        
        # Validate
        val_metrics = validate(
            model=model,
            loader=val_loader,
            criterion=criterion,
            device=device,
            preprocessor=preprocessor,
            rank=rank,
            world_size=world_size
        )
        
        # Update scheduler
        if scheduler is not None and scheduler_type != 'onecycle':
            scheduler.step()
        
        # Logging (rank 0 only)
        if is_main_process(rank):
            writer.add_scalar('train/loss', train_metrics['loss'], epoch)
            writer.add_scalar('train/mae_hours', train_metrics.get('mae_hours', 0), epoch)
            writer.add_scalar('train/rmse_hours', train_metrics.get('rmse_hours', 0), epoch)
            writer.add_scalar('val/loss', val_metrics['loss'], epoch)
            writer.add_scalar('val/mae_hours', val_metrics.get('mae_hours', 0), epoch)
            writer.add_scalar('val/rmse_hours', val_metrics.get('rmse_hours', 0), epoch)
            writer.add_scalar('val/r2', val_metrics.get('r2', 0), epoch)
            writer.add_scalar('learning_rate', current_lr, epoch)
            
            training_history['train_loss'].append(train_metrics['loss'])
            training_history['val_loss'].append(val_metrics['loss'])
            training_history['train_mae_hours'].append(train_metrics.get('mae_hours', 0))
            training_history['val_mae_hours'].append(val_metrics.get('mae_hours', 0))
            training_history['train_rmse_hours'].append(train_metrics.get('rmse_hours', 0))
            training_history['val_rmse_hours'].append(val_metrics.get('rmse_hours', 0))
            training_history['val_r2'].append(val_metrics.get('r2', 0))
            training_history['learning_rate'].append(current_lr)
        
        epoch_time = (datetime.now() - epoch_start_time).total_seconds()
        
        print_rank0(f"Train - Loss: {train_metrics['loss']:.4f}, "
                   f"MAE: {train_metrics.get('mae_hours', 0):.2f}h, "
                   f"RMSE: {train_metrics.get('rmse_hours', 0):.2f}h", rank)
        print_rank0(f"Val   - Loss: {val_metrics['loss']:.4f}, "
                   f"MAE: {val_metrics.get('mae_hours', 0):.2f}h, "
                   f"RMSE: {val_metrics.get('rmse_hours', 0):.2f}h, "
                   f"R²: {val_metrics.get('r2', 0):.4f}", rank)
        print_rank0(f"Time: {epoch_time:.1f}s", rank)
        
        # Save best model (rank 0 only)
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']
            best_val_mae_hours = val_metrics.get('mae_hours', 0)
            best_epoch = epoch
            
            save_checkpoint(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                metrics={
                    'val_loss': val_metrics['loss'],
                    'val_mae_hours': val_metrics.get('mae_hours', 0),
                    'val_rmse_hours': val_metrics.get('rmse_hours', 0),
                    'val_r2': val_metrics.get('r2', 0)
                },
                model_config=model_config,
                vocab_sizes=vocab_sizes,
                save_path=os.path.join(save_dir, 'best_model.pt'),
                rank=rank
            )
            print_rank0(f"✓ New best model saved (val_loss: {best_val_loss:.4f}, MAE: {best_val_mae_hours:.2f}h)", rank)
        
        # Save periodic checkpoint
        if (epoch + 1) % save_every == 0:
            save_checkpoint(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                metrics={
                    'val_loss': val_metrics['loss'],
                    'val_mae_hours': val_metrics.get('mae_hours', 0)
                },
                model_config=model_config,
                vocab_sizes=vocab_sizes,
                save_path=os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt'),
                rank=rank
            )
            print_rank0(f"✓ Checkpoint saved at epoch {epoch+1}", rank)
        
        # Early stopping check (all ranks need to agree)
        if early_stopping(val_metrics['loss']):
            print_rank0(f"\n{'='*50}", rank)
            print_rank0(f"Early stopping triggered after {epoch+1} epochs", rank)
            print_rank0(f"Best epoch: {best_epoch+1} with val_loss: {best_val_loss:.4f}", rank)
            print_rank0(f"{'='*50}", rank)
            break
        
        # Synchronize at end of epoch
        dist.barrier()
    
    # Save training history
    if is_main_process(rank):
        with open(os.path.join(save_dir, 'training_history.json'), 'w') as f:
            json.dump(training_history, f, indent=2)
        
        # Save final model
        save_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=epoch,
            metrics={
                'val_loss': val_metrics['loss'],
                'val_mae_hours': val_metrics.get('mae_hours', 0)
            },
            model_config=model_config,
            vocab_sizes=vocab_sizes,
            save_path=os.path.join(save_dir, 'final_model.pt'),
            rank=rank
        )
    
    # Synchronize before testing
    dist.barrier()
    
    # ========================================
    # Testing
    # ========================================
    
    print_rank0("\n" + "="*80, rank)
    print_rank0("STEP 7: Testing on best model", rank)
    print_rank0("="*80, rank)
    
    # Load best model on all ranks
    checkpoint = load_checkpoint(
        os.path.join(save_dir, 'best_model.pt'),
        model=model,
        device=device
    )
    
    print_rank0(f"Loaded best model from epoch {checkpoint['epoch']+1}", rank)
    
    # Test
    test_metrics = validate(
        model=model,
        loader=test_loader,
        criterion=criterion,
        device=device,
        preprocessor=preprocessor,
        rank=rank,
        world_size=world_size
    )
    
    print_rank0(f"\nTest Results:", rank)
    print_rank0(f"  Loss: {test_metrics['loss']:.4f}", rank)
    print_rank0(f"  MAE (scaled): {test_metrics.get('mae_scaled', 0):.4f}", rank)
    print_rank0(f"  RMSE (scaled): {test_metrics.get('rmse_scaled', 0):.4f}", rank)
    print_rank0(f"  MAE (hours): {test_metrics.get('mae_hours', 0):.2f}", rank)
    print_rank0(f"  RMSE (hours): {test_metrics.get('rmse_hours', 0):.2f}", rank)
    print_rank0(f"  MAPE: {test_metrics.get('mape', 0):.2f}%", rank)
    print_rank0(f"  R²: {test_metrics.get('r2', 0):.4f}", rank)
    
    # Save test results (rank 0 only)
    if is_main_process(rank):
        test_results = {
            'best_epoch': best_epoch + 1,
            'best_val_loss': float(best_val_loss),
            'best_val_mae_hours': float(best_val_mae_hours),
            'test_metrics': {k: float(v) for k, v in test_metrics.items()},
            'training_epochs': epoch + 1,
            'early_stopped': early_stopping.counter >= early_stopping.patience,
            'world_size': world_size
        }
        
        with open(os.path.join(save_dir, 'test_results.json'), 'w') as f:
            json.dump(test_results, f, indent=2)
        
        writer.close()
    
    # ========================================
    # Cleanup
    # ========================================
    
    print_rank0("\n" + "="*80, rank)
    print_rank0("Distributed Training Complete!", rank)
    print_rank0("="*80, rank)
    print_rank0(f"Best epoch: {best_epoch+1}", rank)
    print_rank0(f"Best val loss: {best_val_loss:.4f}", rank)
    print_rank0(f"Best val MAE: {best_val_mae_hours:.2f} hours", rank)
    print_rank0(f"Test MAE: {test_metrics.get('mae_hours', 0):.2f} hours", rank)
    print_rank0(f"Test R²: {test_metrics.get('r2', 0):.4f}", rank)
    print_rank0(f"\nResults saved to: {save_dir}", rank)
    print_rank0("="*80, rank)
    
    cleanup_distributed()
    
    if is_main_process(rank):
        return {
            'save_dir': save_dir,
            'best_epoch': best_epoch,
            'best_val_loss': best_val_loss,
            'test_metrics': test_metrics,
            'training_history': training_history
        }


if __name__ == '__main__':
    main()